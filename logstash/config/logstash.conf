input {
  file {
	path => "/etc/logstash/conf.d/Data/*.csv"
	start_position => "beginning"
    sincedb_path => "/dev/null"
  }
}
filter {
    csv {
		columns => ["Plugin ID", "CVE", "CVSS","Risk", "Host","Protocol", "Port", "Name"]
		separator => ","
		remove_field => ["message"]
    }
	mutate {
		gsub => ["CVSS", ",", "."]
        uppercase => [ "Protocol" ]
		convert => {
            "CVSS" => "float"
            "Port" => "integer"
			"Plugin ID" => "integer"
        }
	}
	grok {
		match => ["path", "%{GREEDYDATA}/%{GREEDYDATA:filename}\.csv"]
		add_tag => [ filename ]
	}
	geoip {
		source => "Host"
  }
}
output {
        elasticsearch {
				action => "index"
                hosts => "elasticsearch:9200"
				index => "logstash-%{+YYYY.MM.dd}"
				workers => 1
        }
}
